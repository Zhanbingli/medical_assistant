import streamlit as st
import ollama
import chromadb
import uuid
import os
from sentence_transformers import CrossEncoder
import time

# === 1. åŸºç¡€é…ç½®ä¸æ•°æ®åº“åˆå§‹åŒ– ===
st.set_page_config(page_title="AI å¾ªè¯åŒ»å­¦åŠ©æ‰‹", layout="wide")

@st.cache_resource
def init_memory():
    # æ•°æ®æŒä¹…åŒ–å­˜å‚¨
    client = chromadb.PersistentClient(path="./medical_db")
    collection = client.get_or_create_collection(name="medical_knowledge")
    return collection

memory_collection = init_memory()

@st.cache_resource
def init_reranker():
    print("Loading Rerank model...")
    return CrossEncoder('BAAI/bge-reranker-base')
reranker = init_reranker()


# === 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ===

def get_existing_files():
    """è·å–æ•°æ®åº“ä¸­å·²å­˜å‚¨çš„æ‰€æœ‰æ–‡ä»¶å"""
    try:
        data = memory_collection.get(include=['metadatas'])
        if not data['metadatas']:
            return set()
        files = set([m.get('source') for m in data['metadatas'] if m])
        return files
    except Exception:
        return set()

def delete_file_from_db(filename):
    """ä»æ•°æ®åº“ä¸­åˆ é™¤æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰ç‰‡æ®µ"""
    try:
        memory_collection.delete(where={"source": filename})
        return True
    except Exception as e:
        return str(e)

def split_markdown_smart(text, chunk_size=600): # ä¿®æ­£å‡½æ•°åæ‹¼å†™
    lines = text.split('\n')
    chunks = []
    current_chunk = []
    current_length = 0
    current_headers = []

    for line in lines:
        stripped = line.strip()
        if stripped.startswith('#'):
            level = stripped.count('#')
            title = stripped.strip('#').strip()
            if len(current_headers) >= level:
                current_headers = current_headers[:level-1]
            current_headers.append(title)

            # æ ‡é¢˜ä¹Ÿä½œä¸ºæ­£æ–‡çš„ä¸€éƒ¨åˆ†ï¼Œä¿è¯ä¸Šä¸‹æ–‡è¿è´¯
            current_chunk.append(line)
            current_length += len(line)
            continue

        current_chunk.append(line)
        current_length += len(line)

        if current_length > chunk_size:
            header_context = " > ".join(current_headers)
            full_text = f"ã€ç« èŠ‚ï¼š{header_context}ã€‘\n" + "\n".join(current_chunk)
            chunks.append(full_text)

            # ç®€å•çš„é‡å ç­–ç•¥ï¼šä¿ç•™æœ€å3è¡Œ
            current_chunk = current_chunk[-3:]
            current_length = sum(len(l) for l in current_chunk)

    if current_chunk:
        header_context = " > ".join(current_headers) # ç»Ÿä¸€åˆ†éš”ç¬¦æ ¼å¼
        full_text = f"ã€ç« èŠ‚ï¼š{header_context}ã€‘\n" + "\n".join(current_chunk)
        chunks.append(full_text)
    return chunks


def save_uploaded_file(uploaded_file):
    """ä¿å­˜æ–‡ä»¶"""
    existing_files = get_existing_files()
    if uploaded_file.name in existing_files:
        return False, "EXIST"

    try:
        content = uploaded_file.read().decode("utf-8")
        raw_chunks = split_markdown_smart(content, chunk_size=600)
        total_chunks = len(raw_chunks)

        if total_chunks == 0: return False, "EMPTY"

        progress_bar = st.progress(0, text=f"æ­£åœ¨å­¦ä¹ æ–°ä¹¦: {uploaded_file.name}...")

        ids_batch, embeddings_batch, documents_batch, metadatas_batch = [], [], [], []
        BATCH_SIZE = 20

        for i, chunk in enumerate(raw_chunks):
            if len(chunk) < 10: continue

            try:
                response = ollama.embeddings(model='bge-m3', prompt=chunk)
                ids_batch.append(str(uuid.uuid4()))
                embeddings_batch.append(response['embedding'])
                documents_batch.append(chunk)
                metadatas_batch.append({"source": uploaded_file.name, "chunk_index": i})

                if len(ids_batch) >= BATCH_SIZE:
                    memory_collection.add(ids=ids_batch, embeddings=embeddings_batch, documents=documents_batch, metadatas=metadatas_batch)
                    ids_batch, embeddings_batch, documents_batch, metadatas_batch = [], [], [], []
            except Exception as e:
                return False, str(e)

            progress_bar.progress((i + 1) / total_chunks)

        if ids_batch:
            memory_collection.add(ids=ids_batch, embeddings=embeddings_batch, documents=documents_batch, metadatas=metadatas_batch)

        progress_bar.empty()
        return True, total_chunks
    except Exception as e:
        return False, str(e)

def generate_search_queries(original_query):
    """ç”Ÿæˆæ‰©å±•æŸ¥è¯¢è¯"""
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æœç´¢ä¼˜åŒ–ä¸“å®¶ã€‚
    è¯·æ ¹æ®ç”¨æˆ·çš„å£è¯­åŒ–æè¿°ï¼Œç”Ÿæˆ 3 ä¸ªç”¨äºæ£€ç´¢åŒ»å­¦æ•™æçš„ä¸“ä¸šå…³é”®è¯æˆ–çŸ­è¯­ã€‚
    ç”¨æˆ·é—®é¢˜: "{original_query}"
    è¦æ±‚:
    1. åŒ…å«åŒ»å­¦æœ¯è¯­ã€‚
    2. åŒ…å«å¯èƒ½çš„å…³è”ç–¾ç—…ã€‚
    3. åªè¾“å‡º 3 è¡Œå…³é”®è¯ï¼Œä¸è¦æœ‰åºå·ã€‚
    """
    try:
        response = ollama.chat(
            model='qwen2.5:7b',
            messages=[{'role': 'user', 'content': prompt}],
            options={'temperature': 0.7}
        )
        queries = response['message']['content'].strip().split('\n')
        clean_queries = [q.split('.')[-1].strip() for q in queries if q.strip()]
        return [original_query] + clean_queries[:3]
    except:
        return [original_query]

def search_memory(query, debug=False):
    """å¤šè·¯å¬å› + Rerank æ ¸å¿ƒå‡½æ•°"""
    debug_logs = []
    try:
        debug_logs.append(f"ğŸ” åŸå§‹æŸ¥è¯¢: {query}")

        # 1. æ‰©å±•æŸ¥è¯¢
        expanded_queries = generate_search_queries(query) # ä¿®æ­£æ‹¼å†™
        if debug:
            debug_logs.append(f"ğŸ§  æ‰©å±•å…³é”®è¯: {expanded_queries}")

        all_documents = []
        all_metadatas = []
        seen_docs = set()

        # 2. å¤šè·¯å¬å›
        for q in expanded_queries:
            try:
                response = ollama.embeddings(model='bge-m3', prompt=q)
                results = memory_collection.query(query_embeddings=[response['embedding']], n_results=5)

                if results['documents'] and results["documents"][0]:
                    docs = results["documents"][0]
                    # å®¹é”™å¤„ç†ï¼šå¦‚æœæ²¡æœ‰ metadataï¼Œå¡«å……ç©ºå­—å…¸
                    metas = results['metadatas'][0] if results['metadatas'] else [{}] * len(docs)

                    for doc, meta in zip(docs, metas):
                        if doc not in seen_docs:
                            all_documents.append(doc)
                            all_metadatas.append(meta)
                            seen_docs.add(doc)
            except Exception as e:
                debug_logs.append(f"âš ï¸ æ£€ç´¢å…³é”®è¯ '{q}' æ—¶å‡ºé”™: {e}")

        if not all_documents:
            return "æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™ã€‚", debug_logs

        debug_logs.append(f"âˆ‘ å…±å¬å› {len(all_documents)} æ¡ä¸é‡å¤ç‰‡æ®µï¼Œå¼€å§‹ Rerank...")

        # 3. é‡æ’ (Rerank)
        pairs = [[query, doc] for doc in all_documents]
        scores = reranker.predict(pairs)
        scored_docs = sorted(zip(all_documents, scores, all_metadatas), key=lambda x: x[1], reverse=True)

        top_k_docs = []

        # 4. ç­›é€‰ä¸æ—¥å¿—
        for doc, score, meta in scored_docs:
            source_name = meta.get('source', 'æœªçŸ¥æ¥æº') if meta else 'æœªçŸ¥æ¥æº'

            # è®°å½•è¯¦ç»†æ—¥å¿—ç”¨äº UI å±•ç¤º
            if debug:
                preview = doc[:20].replace('\n', ' ')
                log_str = f"[{score:.2f}] {source_name}: {preview}..."
                debug_logs.append(log_str)

            # ç­›é€‰é€»è¾‘ï¼šé˜ˆå€¼ -10ï¼Œæœ€å¤šå– 3 æ¡
            if len(top_k_docs) < 3 and score > -10:
                doc_with_source = f"{doc}\n[æ¥æº: {source_name}]"
                top_k_docs.append(doc_with_source)

        if not top_k_docs:
            return "èµ„æ–™ç›¸å…³åº¦è¾ƒä½ï¼Œå»ºè®®è¡¥å……ç»†èŠ‚ã€‚", debug_logs

        return "\n---\n".join(top_k_docs), debug_logs

    except Exception as e:
        return f"æ£€ç´¢è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}", [str(e)]


# === 3. UI ç•Œé¢å¸ƒå±€ ===

with st.sidebar:
    st.header("ğŸ“š çŸ¥è¯†åº“ç®¡å®¶")
    st.subheader("å·²å­¦ä¹ çš„ä¹¦ç±")
    current_files = get_existing_files()

    if not current_files:
        st.caption("æš‚æ— æ•°æ®ï¼Œè¯·ä¸Šä¼ ã€‚")
    else:
        for f in current_files:
            col1, col2 = st.columns([3, 1])
            with col1:
                st.text(f"ğŸ“– {f}")
            with col2:
                if st.button("åˆ ", key=f"del_{f}", help=f"åˆ é™¤ã€Š{f}ã€‹"):
                    res = delete_file_from_db(f)
                    if res is True:
                        st.success(f"å·²åˆ é™¤")
                        st.rerun()
                    else:
                        st.error(f"å¤±è´¥: {res}")

    st.divider()
    debug_mode = st.toggle('å¼€å¯è°ƒè¯•æ¨¡å¼ (Debug)', value=False) # ä¿®æ­£æ–‡æ¡ˆ

    st.subheader("ä¸Šä¼ æ–°ä¹¦")
    uploaded_files = st.file_uploader("æ”¯æŒ Markdown", type=["md"], accept_multiple_files=True)

    if uploaded_files:
        if st.button("å¼€å§‹å­¦ä¹ "):
            for file in uploaded_files:
                with st.spinner(f"æ­£åœ¨å¤„ç† {file.name}..."):
                    success, info = save_uploaded_file(file)
                    if success:
                        st.balloons()
                        st.success(f"å­˜å…¥ {info} æ¡çŸ¥è¯†ç‰‡æ®µã€‚")
                        st.rerun()
                    elif info == "EXIST":
                        st.warning(f"ã€Š{file.name}ã€‹å·²ç»å­¦è¿‡äº†ï¼Œè·³è¿‡ã€‚")
                    else:
                        st.error(f"å¤±è´¥: {info}")

# === 4. ä¸»èŠå¤©ç•Œé¢ ===

st.title("ğŸ‘¨â€âš•ï¸ AI å¾ªè¯åŒ»å­¦åŠ©æ‰‹")

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„åŒ»å­¦åŠ©æ‰‹ã€‚å·²å­¦çŸ¥è¯†è¯·çœ‹å·¦ä¾§åˆ—è¡¨ã€‚"}]

for msg in st.session_state.messages:
    if msg["role"] == "assistant":
        st.chat_message(msg["role"], avatar="ğŸ‘¨â€âš•ï¸").write(msg["content"])
    else:
        st.chat_message(msg["role"], avatar="ğŸ§‘â€ğŸ“").write(msg["content"])

prompt = st.chat_input("è¯·è¾“å…¥ç—…ä¾‹...")

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user", avatar="ğŸ§‘â€ğŸ“").write(prompt)

    with st.chat_message("assistant", avatar="ğŸ‘¨â€âš•ï¸"):
        response_container = st.empty()
        with st.status("æ­£åœ¨æ¨ç†...", expanded=True) as status:

            system_prompt = """
            ä½ æ˜¯ä¸€ä¸ªå¿…é¡»æŸ¥é˜…çŸ¥è¯†åº“çš„åŒ»å­¦AIåŠ©æ‰‹ã€‚

            ã€é“å¾‹ - å¿…é¡»éµå®ˆã€‘ï¼š
            1. **ç¬¬ä¸€æ­¥å¿…é¡»æ˜¯æ£€ç´¢**ï¼šæ— è®ºç”¨æˆ·é—®ä»€ä¹ˆï¼ˆåªè¦å’ŒåŒ»å­¦æœ‰å…³ï¼‰ï¼Œä½ è¾“å‡ºçš„ç¬¬ä¸€å¥è¯å¿…é¡»æ˜¯ "Action: æ£€ç´¢: [å…³é”®è¯]"ã€‚
            2. **ç¦æ­¢è£¸ç­”**ï¼šåœ¨æ²¡æœ‰çœ‹åˆ° Observation (æ£€ç´¢ç»“æœ) ä¹‹å‰ï¼Œç¦æ­¢ç»™å‡ºä»»ä½•å»ºè®®ï¼Œç¦æ­¢åé—®ç”¨æˆ·ã€‚
            3. **å¼ºåˆ¶å…³è”**ï¼šå¦‚æœç”¨æˆ·é—®â€œæ€ä¹ˆæ²»â€ï¼Œè€Œä½ ä¸çŸ¥é“ç—…å› ï¼Œå…ˆæ£€ç´¢ç—‡çŠ¶ï¼ˆå¦‚ "Action: æ£€ç´¢: å‘çƒ­å¯’æˆ˜"ï¼‰æ¥çœ‹çœ‹å¯èƒ½æ˜¯ä»€ä¹ˆç—…ã€‚

            ã€æ ‡å‡†å·¥ä½œæµã€‘ï¼š
            User: å‘çƒ­ä¼´å¯’æˆ˜
            Assistant: Thought: ç”¨æˆ·æåˆ°ç—‡çŠ¶ï¼Œæˆ‘å¿…é¡»å…ˆæŸ¥åº“ã€‚
            Action: æ£€ç´¢: å‘çƒ­ä¼´å¯’æˆ˜
            Observation: (ç³»ç»Ÿè¿”å›çŸ¥è¯†)
            Final Answer: æ ¹æ®èµ„æ–™ï¼Œè¿™å¯èƒ½æ˜¯...
            """

            messages = [{"role": "system", "content": system_prompt}]
            # ä¸Šä¸‹æ–‡è®°å¿†ï¼šå–æœ€å2è½®å¯¹è¯
            for msg in st.session_state.messages[-4:]:
                messages.append(msg)
            messages.append({"role": "user", "content": prompt})

            final_answer = ""
            last_action = ""

            for step in range(5):
                # è°ƒç”¨ LLMï¼ŒTemperature=0 ä¿è¯ä¸¥è°¨
                response = ollama.chat(model='qwen2.5:7b', messages=messages, options={'temperature': 0})
                ai_content = response['message']['content']
                st.markdown(f"*{ai_content}*")
                messages.append(response['message'])

                if "æ£€ç´¢:" in ai_content or "æ£€ç´¢ï¼š" in ai_content:
                    splitter = "æ£€ç´¢:" if "æ£€ç´¢:" in ai_content else "æ£€ç´¢ï¼š"
                    keyword = ai_content.split(splitter)[-1].split("\n")[0].strip()

                    if keyword == last_action:
                        obs = "Observation: å·²æœç´¢è¿‡è¯¥è¯ï¼Œæ— æ–°ä¿¡æ¯ã€‚è¯·å°è¯•æ€»ç»“ã€‚"
                    else:
                        st.info(f"æŸ¥é˜…: {keyword}")

                        # === å…³é”®ä¿®æ­£ï¼šç›´æ¥ä½¿ç”¨ search_memory çš„ç»“æœï¼Œä¸è¦é‡å¤æ£€ç´¢ ===
                        res, logs = search_memory(keyword, debug=debug_mode)

                        # è°ƒè¯•ä¿¡æ¯å±•ç¤º
                        if debug_mode:
                            with st.expander("ğŸ“Š Rerank æ‰“åˆ†è¯¦æƒ…", expanded=True):
                                for log in logs:
                                    # === ä¿®å¤å¼€å§‹ï¼šæ›´å¥å£®çš„æ—¥å¿—è§£æ ===
                                    try:
                                        # 1. åªæœ‰ä»¥ "[" å¼€å¤´çš„æ—¥å¿—æ‰å¯èƒ½æ˜¯æ‰“åˆ†æ—¥å¿— (è¿‡æ»¤æ‰ "ğŸ§  æ‰©å±•å…³é”®è¯" è¿™ç§)
                                        if log.strip().startswith("["):
                                            # æå–åˆ†æ•°
                                            score_str = log.split(']')[0].replace('[', '')
                                            score = float(score_str)

                                            # æ ¹æ®åˆ†æ•°æ˜¾ç¤ºé¢œè‰²
                                            if score > -10:
                                                st.success(log) # é«˜åˆ†ç»¿åº•
                                            else:
                                                st.text(log)    # ä½åˆ†ç°åº•
                                        else:
                                            # 2. å…¶ä»–ç±»å‹çš„æ—¥å¿—ï¼ˆå¦‚æŸ¥è¯¢è¯æ‰©å±•ï¼‰ï¼Œç”¨è“è‰²æ˜¾ç¤º
                                            st.info(log)
                                    except Exception:
                                        # 3. ä¸‡ä¸€è§£æè¿˜æ˜¯å´©äº†ï¼Œå…œåº•æ˜¾ç¤ºçº¯æ–‡æœ¬ï¼Œä¸è®©ç¨‹åºå´©æºƒ
                                        st.text(log)
                        # å°† Rerank åçš„é«˜è´¨é‡å†…å®¹ä¼ ç»™ LLM
                        obs = f"Observation: {res}"
                        last_action = keyword

                    messages.append({"role": "user", "content": obs})

                if "Final Answer" in ai_content:
                    final_answer = ai_content.split("Final Answer")[-1].lstrip(":").lstrip("ï¼š").strip()
                    status.update(label="âœ… å®Œæˆ", state="complete", expanded=False)
                    break

                if "æ£€ç´¢" not in ai_content and len(ai_content) > 20:
                    final_answer = ai_content
                    status.update(label="âœ… å®Œæˆï¼ˆç›´æ¥å›ç­”ï¼‰", state='complete', expanded=False)
                    break

            if not final_answer:
                if len(ai_content) > 10:
                    final_answer = ai_content
                    status.update(label="âš ï¸ å¼ºåˆ¶ç»“æŸï¼ˆå–æœ€åå›å¤ï¼‰", state="complete", expanded=False)
                else:
                    final_answer = "æŠ±æ­‰ï¼Œæˆ‘æœªæŸ¥åˆ°ç›¸å…³èµ„æ–™ï¼Œæœªèƒ½å¾—å‡ºæ˜ç¡®ç»“è®ºã€‚"
                    status.update(label="âŒ æ— ç»“è®º", state="error", expanded=False)

        # æµå¼è¾“å‡ºç»“æœ
        if final_answer:
            def stream_text():
                for word in final_answer.split():
                    yield word + " "
                    time.sleep(0.02)

            response_container.write_stream(stream_text)
            st.session_state.messages.append({"role": "assistant", "content": final_answer})
