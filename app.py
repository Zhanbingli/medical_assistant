"""
AI å¾ªè¯åŒ»å­¦åŠ©æ‰‹ - ä¸»åº”ç”¨
åŸºäº Streamlit çš„åŒ»å­¦çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ
"""
import streamlit as st
import ollama
import time

from config import (
    APP_TITLE, PAGE_LAYOUT,
    DB_PATH, COLLECTION_NAME,
    RERANKER_MODEL, EMBEDDING_MODEL, LLM_MODEL,
    BATCH_SIZE, SYSTEM_PROMPT,
    MAX_REASONING_STEPS, CONTEXT_HISTORY_TURNS,
    LLM_TEMPERATURE_STRICT
)
from database import MedicalKnowledgeDB
from document_processor import DocumentEmbedder
from search import MedicalSearchEngine, Reranker, QueryExpander


# === é¡µé¢é…ç½® ===
st.set_page_config(page_title=APP_TITLE, layout=PAGE_LAYOUT)


# === åˆå§‹åŒ–ç»„ä»¶ï¼ˆç¼“å­˜ï¼‰ ===
@st.cache_resource
def init_components():
    """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶å¹¶ç¼“å­˜"""
    db = MedicalKnowledgeDB(DB_PATH, COLLECTION_NAME)
    reranker = Reranker(RERANKER_MODEL)
    expander = QueryExpander(LLM_MODEL)
    search_engine = MedicalSearchEngine(db, reranker, expander)
    embedder = DocumentEmbedder(EMBEDDING_MODEL, BATCH_SIZE)
    return db, search_engine, embedder


db, search_engine, embedder = init_components()


# === ä¾§è¾¹æ ï¼šçŸ¥è¯†åº“ç®¡ç† ===
with st.sidebar:
    st.header("ğŸ“š çŸ¥è¯†åº“ç®¡å®¶")
    st.subheader("å·²å­¦ä¹ çš„ä¹¦ç±")

    # æ˜¾ç¤ºå·²å­˜å‚¨çš„æ–‡ä»¶
    current_files = db.get_existing_files()

    if not current_files:
        st.caption("æš‚æ— æ•°æ®ï¼Œè¯·ä¸Šä¼ ã€‚")
    else:
        for f in current_files:
            col1, col2 = st.columns([3, 1])
            with col1:
                st.text(f"ğŸ“– {f}")
            with col2:
                if st.button("åˆ ", key=f"del_{f}", help=f"åˆ é™¤ã€Š{f}ã€‹"):
                    success, error = db.delete_file(f)
                    if success:
                        st.success(f"å·²åˆ é™¤")
                        st.rerun()
                    else:
                        st.error(f"å¤±è´¥: {error}")

    st.divider()

    # è°ƒè¯•æ¨¡å¼å¼€å…³
    debug_mode = st.toggle('å¼€å¯è°ƒè¯•æ¨¡å¼ (Debug)', value=False)

    # ä¸Šä¼ æ–°æ–‡ä»¶
    st.subheader("ä¸Šä¼ æ–°ä¹¦")
    uploaded_files = st.file_uploader(
        "æ”¯æŒ Markdown",
        type=["md"],
        accept_multiple_files=True
    )

    if uploaded_files:
        if st.button("å¼€å§‹å­¦ä¹ "):
            for file in uploaded_files:
                with st.spinner(f"æ­£åœ¨å¤„ç† {file.name}..."):
                    # è¯»å–æ–‡ä»¶å†…å®¹
                    content = file.read().decode("utf-8")

                    # å®šä¹‰è¿›åº¦å›è°ƒ
                    progress_bar = st.progress(0, text=f"æ­£åœ¨å­¦ä¹ æ–°ä¹¦: {file.name}...")

                    def update_progress(progress, text):
                        progress_bar.progress(progress, text=text)

                    # å¤„ç†æ–‡ä»¶
                    success, info = embedder.process_file(
                        content, file.name, db, update_progress
                    )

                    progress_bar.empty()

                    # æ˜¾ç¤ºç»“æœ
                    if success:
                        st.balloons()
                        st.success(f"å­˜å…¥ {info} æ¡çŸ¥è¯†ç‰‡æ®µã€‚")
                        st.rerun()
                    elif info == "EXIST":
                        st.warning(f"ã€Š{file.name}ã€‹å·²ç»å­¦è¿‡äº†ï¼Œè·³è¿‡ã€‚")
                    else:
                        st.error(f"å¤±è´¥: {info}")


# === ä¸»èŠå¤©ç•Œé¢ ===
st.title(f"ğŸ‘¨â€âš•ï¸ {APP_TITLE}")

# åˆå§‹åŒ–å¯¹è¯å†å²
if "messages" not in st.session_state:
    st.session_state["messages"] = [{
        "role": "assistant",
        "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„åŒ»å­¦åŠ©æ‰‹ã€‚å·²å­¦çŸ¥è¯†è¯·çœ‹å·¦ä¾§åˆ—è¡¨ã€‚"
    }]

# æ˜¾ç¤ºå¯¹è¯å†å²
for msg in st.session_state.messages:
    if msg["role"] == "assistant":
        st.chat_message(msg["role"], avatar="ğŸ‘¨â€âš•ï¸").write(msg["content"])
    else:
        st.chat_message(msg["role"], avatar="ğŸ§‘â€ğŸ“").write(msg["content"])

# ç”¨æˆ·è¾“å…¥
prompt = st.chat_input("è¯·è¾“å…¥ç—…ä¾‹...")

if prompt:
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user", avatar="ğŸ§‘â€ğŸ“").write(prompt)

    # AI å›å¤
    with st.chat_message("assistant", avatar="ğŸ‘¨â€âš•ï¸"):
        response_container = st.empty()

        with st.status("æ­£åœ¨æ¨ç†...", expanded=True) as status:
            # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
            messages = [{"role": "system", "content": SYSTEM_PROMPT}]

            # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€è¿‘ N è½®ï¼‰
            history_start = max(0, len(st.session_state.messages) - CONTEXT_HISTORY_TURNS * 2)
            for msg in st.session_state.messages[history_start:]:
                messages.append(msg)

            messages.append({"role": "user", "content": prompt})

            # ReAct æ¨ç†å¾ªç¯
            final_answer = ""
            last_action = ""

            for step in range(MAX_REASONING_STEPS):
                # è°ƒç”¨ LLM
                response = ollama.chat(
                    model=LLM_MODEL,
                    messages=messages,
                    options={'temperature': LLM_TEMPERATURE_STRICT}
                )
                ai_content = response['message']['content']
                st.markdown(f"*{ai_content}*")
                messages.append(response['message'])

                # æ£€æµ‹æ£€ç´¢åŠ¨ä½œ
                if "æ£€ç´¢:" in ai_content or "æ£€ç´¢ï¼š" in ai_content:
                    splitter = "æ£€ç´¢:" if "æ£€ç´¢:" in ai_content else "æ£€ç´¢ï¼š"
                    keyword = ai_content.split(splitter)[-1].split("\n")[0].strip()

                    # é¿å…é‡å¤æ£€ç´¢
                    if keyword == last_action:
                        obs = "Observation: å·²æœç´¢è¿‡è¯¥è¯ï¼Œæ— æ–°ä¿¡æ¯ã€‚è¯·å°è¯•æ€»ç»“ã€‚"
                    else:
                        st.info(f"æŸ¥é˜…: {keyword}")

                        # æ‰§è¡Œæœç´¢
                        res, logs = search_engine.search(keyword, debug=debug_mode)

                        # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
                        if debug_mode:
                            with st.expander("ğŸ“Š Rerank æ‰“åˆ†è¯¦æƒ…", expanded=True):
                                for log in logs:
                                    try:
                                        # åªæœ‰ä»¥ "[" å¼€å¤´çš„æ—¥å¿—æ‰å¯èƒ½æ˜¯æ‰“åˆ†æ—¥å¿—
                                        if log.strip().startswith("["):
                                            # æå–åˆ†æ•°
                                            score_str = log.split(']')[0].replace('[', '')
                                            score = float(score_str)

                                            # æ ¹æ®åˆ†æ•°æ˜¾ç¤ºé¢œè‰²
                                            if score > -10:
                                                st.success(log)  # é«˜åˆ†ç»¿åº•
                                            else:
                                                st.text(log)  # ä½åˆ†ç°åº•
                                        else:
                                            # å…¶ä»–ç±»å‹çš„æ—¥å¿—ï¼ˆå¦‚æŸ¥è¯¢è¯æ‰©å±•ï¼‰ï¼Œç”¨è“è‰²æ˜¾ç¤º
                                            st.info(log)
                                    except Exception:
                                        # ä¸‡ä¸€è§£æè¿˜æ˜¯å´©äº†ï¼Œå…œåº•æ˜¾ç¤ºçº¯æ–‡æœ¬
                                        st.text(log)

                        # å°†æ£€ç´¢ç»“æœä¼ ç»™ LLM
                        obs = f"Observation: {res}"
                        last_action = keyword

                    messages.append({"role": "user", "content": obs})

                # æ£€æµ‹æœ€ç»ˆç­”æ¡ˆ
                if "Final Answer" in ai_content:
                    final_answer = ai_content.split("Final Answer")[-1].lstrip(":").lstrip("ï¼š").strip()
                    status.update(label="âœ… å®Œæˆ", state="complete", expanded=False)
                    break

                # ç›´æ¥å›ç­”ï¼ˆæœªä½¿ç”¨æ£€ç´¢ï¼‰
                if "æ£€ç´¢" not in ai_content and len(ai_content) > 20:
                    final_answer = ai_content
                    status.update(label="âœ… å®Œæˆï¼ˆç›´æ¥å›ç­”ï¼‰", state='complete', expanded=False)
                    break

            # å…œåº•å¤„ç†
            if not final_answer:
                if len(ai_content) > 10:
                    final_answer = ai_content
                    status.update(label="âš ï¸ å¼ºåˆ¶ç»“æŸï¼ˆå–æœ€åå›å¤ï¼‰", state="complete", expanded=False)
                else:
                    final_answer = "æŠ±æ­‰ï¼Œæˆ‘æœªæŸ¥åˆ°ç›¸å…³èµ„æ–™ï¼Œæœªèƒ½å¾—å‡ºæ˜ç¡®ç»“è®ºã€‚"
                    status.update(label="âŒ æ— ç»“è®º", state="error", expanded=False)

        # æµå¼è¾“å‡ºç»“æœ
        if final_answer:
            def stream_text():
                for word in final_answer.split():
                    yield word + " "
                    time.sleep(0.02)

            response_container.write_stream(stream_text)
            st.session_state.messages.append({
                "role": "assistant",
                "content": final_answer
            })
